{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d0f368",
   "metadata": {},
   "source": [
    "## TFRecord_0428 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8466c1",
   "metadata": {},
   "source": [
    "- 전처리 x\n",
    "- 증식 x\n",
    "- row 파일 그대로 TFRecord로 생성\n",
    "\n",
    "=> 나중에 normalization과 resize 수행  \n",
    "=> 전체파일크기 맞춰주기는 필요해보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2919e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "############### Parameter\n",
    "NUM_OF_TFRECORDS = 100 # 종류별 TFRecord의 개수\n",
    "BUFFER_SIZE = 16     # 데이터 shuffle을 위한 buffer size\n",
    "BATCH_SIZE = 8       # 배치 사이즈. 한번에 가져오는 이미지 데이터 개수 \n",
    "NUM_CLASS = 88        # class의 개수. binary인 경우는 필요없으며 categorical인 경우 설정\n",
    "IMAGE_SIZE = 150       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93cc3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 필요한 DataSet 준비(여러개의 tfrecord 처리)\n",
    "\n",
    "# train, validation TFRecord 폴더 경로(여러개의 tfrecord 처리)\n",
    "# 폴더를 나누고 파일을 복사하는건 수동으로 처리\n",
    "train_tfrecord_path = './tfrecords_0428/train'\n",
    "valid_tfrecord_path = './tfrecords_0428/valid'\n",
    "\n",
    "train_tfrecord_list = tf.io.gfile.glob(train_tfrecord_path + '/*.tfrecords')\n",
    "valid_tfrecord_list = tf.io.gfile.glob(valid_tfrecord_path + '/*.tfrecords')\n",
    "\n",
    "# 읽어들인 TFRecord를 다음의 형태(dict)로 변환하는 함수\n",
    "# <ParallelMapDataset shapes: {id: (), image_raw: (), label: ()}, \n",
    "#                     types: {id: tf.string, image_raw: tf.string, label: tf.int64}>\n",
    "def _parse_image_function(example_proto):\n",
    "    # TFRecord를 읽어서 데이터를 복원하기 위한 자료구조.\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/channel': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, \n",
    "                                      image_feature_description)\n",
    "\n",
    "# 위에서 얻은 ParallelMapDataset를 다음의 형태(shape)로 변환하는 함수\n",
    "# <ParallelMapDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int64)>\n",
    "def map_func(target_record):      \n",
    "    img = target_record['image/image_raw']\n",
    "    label = target_record['image/label']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)    \n",
    "    return img, label\n",
    "\n",
    "\n",
    "# 전처리(normalization & resize) 함수\n",
    "# 이미지 데이터 normalization\n",
    "# 우리예제는 TFRecord 생성 시 원본 size로 저장했기 때문에 image resize를 해야함.\n",
    "def image_preprocess_func(image, label):\n",
    "    result_image = image / 255\n",
    "    result_image = tf.image.resize(result_image, \n",
    "                                   (IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                   antialias=False)   \n",
    "    return result_image, label\n",
    "\n",
    "\n",
    "# 만약 multinomial classification이면 one_hot처리도 필요함.\n",
    "def image_postprocess_func(image, label):\n",
    "    onehot_label = tf.one_hot(label, depth=88)    # binary인 경우 one_hot 사용안함.    \n",
    "    return image, label\n",
    "\n",
    "def make_dataset(tfrecords_path, is_train):\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_path)\n",
    "\n",
    "    dataset = dataset.map(_parse_image_function,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.map(map_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    dataset = dataset.map(image_preprocess_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.map(image_postprocess_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2813318a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66747bc5fa4247c88398d9891c3c5141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\tfrecords_0428\\train\\0_0001_of_0008.tfrecords\n",
      "<PrefetchDataset shapes: ((None, 150, 150, 3), (None,)), types: (tf.float32, tf.int64)>\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00000, saving model to ./checkpoint_0428\\ad-000001-0.000000-0.000000.hdf5\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.00000\n",
      ".\\tfrecords_0428\\train\\0_0002_of_0008.tfrecords\n",
      "<PrefetchDataset shapes: ((None, 150, 150, 3), (None,)), types: (tf.float32, tf.int64)>\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.00000\n",
      ".\\tfrecords_0428\\train\\0_0003_of_0008.tfrecords\n",
      "<PrefetchDataset shapes: ((None, 150, 150, 3), (None,)), types: (tf.float32, tf.int64)>\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.00000\n",
      ".\\tfrecords_0428\\train\\0_0004_of_0008.tfrecords\n",
      "<PrefetchDataset shapes: ((None, 150, 150, 3), (None,)), types: (tf.float32, tf.int64)>\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.00000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m     valid_dataset \u001b[38;5;241m=\u001b[39m make_dataset(tfrecord_valid_file, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataset)\n\u001b[1;32m---> 63\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;43;03m#               callbacks = [checkpointer, es, learning_rate_reduction],\u001b[39;49;00m\n\u001b[0;32m     67\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m       \n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m############### 기본학습 종료 ###############\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:108\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1086\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# Handle fault-tolerance for multi-worker.\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# TODO(omalleyt): Fix the ordering issues that mean this has to\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;66;03m# happen after `callbacks.on_train_begin`.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m data_handler\u001b[38;5;241m.\u001b[39m_initial_epoch \u001b[38;5;241m=\u001b[39m (  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[1;32m-> 1086\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():\n\u001b[0;32m   1087\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[0;32m   1088\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1143\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1141\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter\u001b[38;5;241m.\u001b[39mshould_recreate_iterator():\n\u001b[1;32m-> 1143\u001b[0m   data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m epoch, data_iterator\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:415\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"Creates an iterator for elements of this dataset.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03mThe returned iterator implements the Python Iterator protocol.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m  RuntimeError: If not inside of tf.function and not executing eagerly.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m--> 415\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__iter__() is only supported inside of tf.function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (_device_stack_is_empty() \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    694\u001b[0m     context\u001b[38;5;241m.\u001b[39mcontext()\u001b[38;5;241m.\u001b[39mdevice_spec\u001b[38;5;241m.\u001b[39mdevice_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    695\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 696\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    698\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_iterator(dataset)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:702\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m    701\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m   \u001b[38;5;66;03m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[39;00m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;66;03m# is being used. For example, `tf.data.Dataset.from_generator` registers\u001b[39;00m\n\u001b[0;32m    706\u001b[0m   \u001b[38;5;66;03m# a few py_funcs that are needed in `self._next_internal`.  If the dataset\u001b[39;00m\n\u001b[0;32m    707\u001b[0m   \u001b[38;5;66;03m# is deleted, this iterator crashes on `self.__next__(...)` call.\u001b[39;00m\n\u001b[0;32m    708\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m dataset\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:386\u001b[0m, in \u001b[0;36mDatasetV2._apply_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.data graph rewrites are not compatible with tf.Variable. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following rewrites will be disabled: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. To enable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewrites, use resource variables instead by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.enable_resource_variables()` at the start of the program.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(graph_rewrites))\n\u001b[0;32m    385\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_OptimizeDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_rewrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mgraph_rewrite_configs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# (3) Apply autotune options\u001b[39;00m\n\u001b[0;32m    390\u001b[0m autotune, algorithm, cpu_budget \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39m_autotune_settings()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4396\u001b[0m, in \u001b[0;36m_OptimizeDataset.__init__\u001b[1;34m(self, input_dataset, optimizations, optimization_configs)\u001b[0m\n\u001b[0;32m   4393\u001b[0m   optimization_configs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   4394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizations \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m   4395\u001b[0m     optimizations, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mstring, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4396\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4397\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   4398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4399\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimization_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimization_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4400\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_structure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4401\u001b[0m \u001b[38;5;28msuper\u001b[39m(_OptimizeDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machine_TF2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3943\u001b[0m, in \u001b[0;36moptimize_dataset\u001b[1;34m(input_dataset, optimizations, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[0;32m   3941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3942\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3943\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3944\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptimizeDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3945\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_callbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3946\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimization_configs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3947\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimization_configs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3949\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############### Model\n",
    "\n",
    "input_layer = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='input_layer')  \n",
    "\n",
    "## Pretrained Network\n",
    "pretrained_model = VGG16(weights='imagenet',\n",
    "                         include_top=False,\n",
    "                         input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                         input_tensor=input_layer)\n",
    "\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "x = pretrained_model.output\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(512,\n",
    "          activation='relu')(x)\n",
    "x = Dense(88, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "\n",
    "# EarlyStopping & Checkpoint & Learning Rate\n",
    "learning_rate_reduction=ReduceLROnPlateau(\n",
    "                        monitor= \"val_acc\", \n",
    "                        patience = 3, \n",
    "                        factor = 0.5, \n",
    "                        min_lr=1e-7,\n",
    "                        verbose=1)\n",
    "\n",
    "model_filename = './checkpoint_0428/ad-{epoch:06d}-{val_acc:0.6f}-{acc:0.6f}.hdf5'\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=model_filename, \n",
    "    verbose=1, \n",
    "    save_freq='epoch', \n",
    "    save_best_only=True, \n",
    "    monitor='val_acc')\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', verbose=1, patience=5)\n",
    "\n",
    "# LearningRateScheduler 이용\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 세팅 후 학습(여러개(100개)의 tfrecord를 이용한 학습)\n",
    "for i in tqdm(range(NUM_OF_TFRECORDS),\n",
    "              total=NUM_OF_TFRECORDS,\n",
    "              position=0,\n",
    "              leave=True):\n",
    "    tfrecord_train_file = train_tfrecord_list[i]\n",
    "    tfrecord_valid_file = valid_tfrecord_list[i]\n",
    "    \n",
    "    print(tfrecord_train_file)\n",
    "    \n",
    "    dataset = make_dataset(tfrecord_train_file, True)\n",
    "    valid_dataset = make_dataset(tfrecord_valid_file, False)\n",
    "    print(dataset)\n",
    "    \n",
    "    model.fit(dataset,\n",
    "              epochs=20,\n",
    "              validation_data=valid_dataset,\n",
    "#               callbacks = [checkpointer, es, learning_rate_reduction],\n",
    "              callbacks = [checkpointer],\n",
    "              verbose=0)       \n",
    "\n",
    "print('############### 기본학습 종료 ###############')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지가 기본학습 처리입니다.\n",
    "\n",
    "# Pretrained Network 위에 새로운 Network을 추가합니다. \n",
    "# Base Network을 동결합니다.\n",
    "# 새로 추가한 Network을 학습합니다.\n",
    "\n",
    "\n",
    "# 아래의 작업이 추가로 필요합니다.\n",
    "# Base Network에서 일부 Layer의 동결을 해제합니다.\n",
    "# 동결을 해제한 층과 새로 추가한 층을 함께 학습합니다.\n",
    "\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    if layer.name in ['block5_conv1','block5_conv2','block5_conv3']:\n",
    "        layer.trainable = True\n",
    "    else:    \n",
    "        layer.trainable = False\n",
    "\n",
    "## learning rate를 줄이는게 일반적(미세조절)        \n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    \n",
    "## 재학습 진행\n",
    "print('########### 재학습 진행 ###########')\n",
    "\n",
    "for i in tqdm(range(NUM_OF_TFRECORDS),\n",
    "              total=NUM_OF_TFRECORDS,\n",
    "              position=0,\n",
    "              leave=True):\n",
    "\n",
    "    tfrecord_train_file = train_tfrecord_list[i]\n",
    "    tfrecord_valid_file = valid_tfrecord_list[i]\n",
    "    \n",
    "    dataset = make_dataset(tfrecord_train_file, True)\n",
    "    valid_dataset = make_dataset(tfrecord_valid_file, False)\n",
    "    \n",
    "    model.fit(dataset,\n",
    "              epochs=20,\n",
    "              validation_data=valid_dataset,\n",
    "#               callbacks = [checkpointer, es, learning_rate_reduction],\n",
    "              callbacks = [checkpointer],\n",
    "              verbose=0)            \n",
    "\n",
    "# 결과그래프(loss, accuracy)를 그리는것은 한번 고민해보시기 바랍니다.!!\n",
    "\n",
    "# 프로그램 수행환경 \n",
    "# Server : AWS EC2 2.4.1\n",
    "#          1 GPU(Tesla T4), 16GB GPU Memory\n",
    "# CUDA : 11.0.4, cuDNN : 8\n",
    "# Tensorflow-gpu : 2.4.1\n",
    " \n",
    "# 최종 저장된 모델\n",
    "\n",
    "# 5개의 TFRecord로 학습한 경우\n",
    "# Epoch 00016: val_acc improved from 0.95300 to 0.95400, \n",
    "# saving model to ./checkpoint/cat-dog-000016-0.954000-1.000000.hdf5\n",
    "\n",
    "# 1개의 TFRecord로 학습한 경우\n",
    "# Epoch 00015: val_acc improved from 0.95140 to 0.95420, \n",
    "# saving model to ./checkpoint/cat-dog-000015-0.954200-0.995150.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd48f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF2] *",
   "language": "python",
   "name": "conda-env-machine_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
